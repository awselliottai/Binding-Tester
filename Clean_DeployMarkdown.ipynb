{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to initialize the ChromeDriver and open the URL\n",
    "def initialize_driver(url):\n",
    "    chrome_options = Options()\n",
    "    # Enable headless mode for faster scraping\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "    # Path to ChromeDriver (adjust the path based on your setup)\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "    except TimeoutException:\n",
    "        print(f\"Failed to load {url}\")\n",
    "        driver.quit()\n",
    "        return None\n",
    "    return driver\n",
    "\n",
    "# Function to extract and summarize the website structure\n",
    "def extract_website_structure(url):\n",
    "    driver = initialize_driver(url)\n",
    "    if not driver:\n",
    "        return \"Error: Could not initialize driver or load the page.\"\n",
    "\n",
    "    # Extract page content using BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Close the driver once we have the page source\n",
    "    driver.quit()\n",
    "\n",
    "    # Summarizing key elements\n",
    "    summary = {\n",
    "        'title': soup.title.string if soup.title else 'No title found',\n",
    "        'headings': [],\n",
    "        'links': [],\n",
    "        'paragraphs': [],\n",
    "        'divs': []\n",
    "    }\n",
    "\n",
    "    # Extract headings (h1, h2, h3, etc.)\n",
    "    for heading_tag in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n",
    "        headings = soup.find_all(heading_tag)\n",
    "        for heading in headings:\n",
    "            summary['headings'].append({heading_tag: heading.get_text(strip=True)})\n",
    "\n",
    "    # Extract all anchor tags and their URLs\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        summary['links'].append({'text': link.get_text(strip=True), 'url': link['href']})\n",
    "\n",
    "    # Extract paragraphs without character limitations\n",
    "    for paragraph in soup.find_all('p'):\n",
    "        summary['paragraphs'].append(paragraph.get_text(strip=True))\n",
    "\n",
    "    # Extract divs with class and ID to get more detailed structure information (without limits on text)\n",
    "    for div in soup.find_all('div'):\n",
    "        class_name = div.get('class', 'N/A')\n",
    "        div_id = div.get('id', 'N/A')\n",
    "        text = div.get_text(strip=True)  # Removed character limit for div text\n",
    "        summary['divs'].append({'class': class_name, 'id': div_id, 'text': text})\n",
    "\n",
    "    return summary\n",
    "\n",
    "# Function to print the summary in grouped sections\n",
    "def print_summary(summary):\n",
    "    print(\"\\nWebsite Structure Summary:\\n\")\n",
    "    \n",
    "    # Print Title\n",
    "    print(f\"Title: {summary['title']}\\n\")\n",
    "    \n",
    "    # Print Headings (grouped by heading tag, e.g., h1, h2, etc.)\n",
    "    print(\"Headings:\")\n",
    "    for heading in summary['headings']:\n",
    "        for tag, text in heading.items():\n",
    "            print(f\"  {tag.upper()}: {text}\")\n",
    "\n",
    "    # Print Links with their URLs\n",
    "    print(\"\\nLinks:\")\n",
    "    for link in summary['links']:\n",
    "        print(f\"  Text: {link['text']}, URL: {link['url']}\")\n",
    "\n",
    "    # Print all paragraphs without limiting them\n",
    "    print(\"\\nParagraphs:\")\n",
    "    for i, paragraph in enumerate(summary['paragraphs']):\n",
    "        print(f\"  Paragraph {i+1}: {paragraph}\")\n",
    "\n",
    "    # Print divs with class, ID, and their text\n",
    "    print(\"\\nDivs with classes and IDs:\")\n",
    "    for i, div in enumerate(summary['divs']):\n",
    "        print(f\"  Class: {div['class']}, ID: {div['id']}, Text: {div['text'][:100]}...\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Ask the user to input the URL\n",
    "    url = input(\"Please enter the URL: \")  # Dynamic URL input\n",
    "    \n",
    "    summary = extract_website_structure(url)\n",
    "    \n",
    "    if summary:\n",
    "        print_summary(summary)\n",
    "\n",
    "\n",
    "        # Write the formatted text to a file\n",
    "        with open(\"cleaned_text.txt\", \"w\") as file:\n",
    "            for key, values in summary.items():\n",
    "                file.write(f\"{key}:\\n\")\n",
    "                for value in values:\n",
    "                    file.write(f\"  - {value}\\n\")\n",
    "                file.write(\"-\" * 40 + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221ad261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 1: Remove HTML tags using BeautifulSoup\n",
    "def remove_html_tags(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    return soup.get_text()\n",
    "\n",
    "# Step 2: Remove paragraph markers like 'Paragraph X:'\n",
    "def remove_redundant_phrases(text):\n",
    "    return re.sub(r'Paragraph \\d+:', '', text)\n",
    "\n",
    "# Step 3: Remove duplicate lines or phrases (this removes repeated sentences)\n",
    "def remove_duplicate_lines(text):\n",
    "    lines = text.splitlines()\n",
    "    unique_lines = list(dict.fromkeys(lines))  # Use dict.fromkeys() to maintain order and remove duplicates\n",
    "    return \" \".join(unique_lines)\n",
    "\n",
    "# Step 4: Clean up extra spaces, newlines, and normalize text\n",
    "def normalize_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces and newlines\n",
    "    text = text.lower()  # Optional: convert to lowercase for consistency\n",
    "    return text\n",
    "\n",
    "# Full pipeline to clean the scraped raw HTML content\n",
    "def clean_scraped_data(raw_html_content):\n",
    "    text = remove_html_tags(raw_html_content)\n",
    "    text = remove_redundant_phrases(text)\n",
    "    text = remove_duplicate_lines(text)\n",
    "    cleaned_text = normalize_text(text)\n",
    "    return cleaned_text\n",
    "\n",
    "# Function to write cleaned data to a file\n",
    "def write_cleaned_data_to_file(cleaned_data, filename=\"cleaned_text.txt\"):\n",
    "    with open(filename, \"w\") as file:\n",
    "        file.write(cleaned_data)\n",
    "\n",
    "# Function to open and read the cleaned data from the file\n",
    "def read_cleaned_data_from_file(filename=\"cleaned_text.txt\"):\n",
    "    with open(filename, \"r\") as file:\n",
    "        return file.read()\n",
    "\n",
    "# Example usage: Write to and Read from 'cleaned_text.txt'\n",
    "if __name__ == \"__main__\":\n",
    "    # Assume that the raw HTML content was processed earlier and saved to 'cleaned_text.txt'\n",
    "    # Now we want to read the cleaned text from the file\n",
    "\n",
    "    # Step 1: Read the cleaned data from the file\n",
    "    cleaned_data_from_file = read_cleaned_data_from_file()\n",
    "\n",
    "    # Step 2: Show the cleaned data (for debugging, remove in production)\n",
    "    print(\"Cleaned Data Read from File:\")\n",
    "    print(cleaned_data_from_file)\n",
    "\n",
    "    # If further cleaning is required, you can perform additional steps here.\n",
    "    # This assumes that `clean_scraped_data` has already been applied and written to 'cleaned_text.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481174fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 1: Remove HTML tags using BeautifulSoup\n",
    "def remove_html_tags(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    return soup.get_text()\n",
    "\n",
    "# Step 2: Remove paragraph markers like 'Paragraph X:'\n",
    "def remove_redundant_phrases(text):\n",
    "    return re.sub(r'Paragraph \\d+:', '', text)\n",
    "\n",
    "# Step 3: Remove duplicate lines or phrases (this removes repeated sentences)\n",
    "def remove_duplicate_lines(text):\n",
    "    lines = text.splitlines()\n",
    "    unique_lines = list(dict.fromkeys(lines))  # Use dict.fromkeys() to maintain order and remove duplicates\n",
    "    return \" \".join(unique_lines)\n",
    "\n",
    "# Step 4: Clean up extra spaces, newlines, and normalize text\n",
    "def normalize_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces and newlines\n",
    "    text = text.lower()  # Optional: convert to lowercase for consistency\n",
    "    return text\n",
    "\n",
    "# Full pipeline to clean the scraped raw HTML content\n",
    "def clean_scraped_data(raw_html_content):\n",
    "    text = remove_html_tags(raw_html_content)\n",
    "    text = remove_redundant_phrases(text)\n",
    "    text = remove_duplicate_lines(text)\n",
    "    cleaned_text = normalize_text(text)\n",
    "    return cleaned_text\n",
    "\n",
    "# Function to open and read the raw HTML content from the file\n",
    "def read_raw_content_from_file(filename=\"cleaned_text.txt\"):\n",
    "    with open(filename, \"r\") as file:\n",
    "        return file.read()\n",
    "\n",
    "# Example usage within your framework\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Import the raw content from the .txt file generated in the previous step\n",
    "    raw_html_content = read_raw_content_from_file(\"cleaned_text.txt\")\n",
    "\n",
    "    # Step 2: Clean the imported raw content\n",
    "    cleaned_data = clean_scraped_data(raw_html_content)\n",
    "\n",
    "    # Step 3: Show the cleaned data for verification (optional)\n",
    "    print(\"Cleaned Data from File:\")\n",
    "    print(cleaned_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc31817b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to open and read the cleaned data from the file\n",
    "def read_cleaned_data_from_file(filename=\"cleaned_text.txt\"):\n",
    "    with open(filename, \"r\") as file:\n",
    "        return file.read()\n",
    "\n",
    "# Step 1: Read the cleaned text from the file\n",
    "cleaned_text = read_cleaned_data_from_file(\"cleaned_text.txt\")\n",
    "\n",
    "# Step 2: Process the text with spaCy\n",
    "doc = nlp(cleaned_text)\n",
    "\n",
    "# Regex patterns for phone numbers, emails, and links\n",
    "phone_regex = r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}'\n",
    "email_regex = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "link_regex = r'https?://[^\\s]+'\n",
    "\n",
    "# Function to extract relevant labeled information\n",
    "def extract_relevant_information(doc):\n",
    "    extracted_info = defaultdict(list)\n",
    "    \n",
    "    # Extract entities using spaCy NER\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            extracted_info['Names'].append(ent.text)\n",
    "        elif ent.label_ == \"ORG\":\n",
    "            extracted_info['Organizations'].append(ent.text)\n",
    "        elif ent.label_ == \"GPE\":\n",
    "            extracted_info['Addresses'].append(ent.text)\n",
    "\n",
    "    # Extract phone numbers using regex\n",
    "    extracted_info['Phone Numbers'] = re.findall(phone_regex, doc.text)\n",
    "    \n",
    "    # Extract email addresses using regex\n",
    "    extracted_info['Emails'] = re.findall(email_regex, doc.text)\n",
    "    \n",
    "    # Extract links using regex\n",
    "    extracted_info['Links'] = re.findall(link_regex, doc.text)\n",
    "\n",
    "        # Remove duplicates by converting lists to sets and back to lists\n",
    "    for key in extracted_info:\n",
    "        extracted_info[key] = list(set(extracted_info[key]))\n",
    "    \n",
    "    return extracted_info\n",
    "\n",
    "# Extract the relevant information\n",
    "extracted_info = extract_relevant_information(doc)\n",
    "\n",
    "# Function to print the extracted information\n",
    "def print_extracted_info(extracted_info):\n",
    "    for key, values in extracted_info.items():\n",
    "        print(f\"{key}:\")\n",
    "        for value in values:\n",
    "            print(f\"  - {value}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Step 3: Print the extracted and labeled information\n",
    "print_extracted_info(extracted_info)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
